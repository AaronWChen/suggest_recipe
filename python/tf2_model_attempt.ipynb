{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../raw_data/train.json\"\n",
    "save_path = \"../write_data\"\n",
    "embedding_size = 100\n",
    "epochs_desired = 15\n",
    "learning_rate = 0.025\n",
    "regularization = 0.01\n",
    "algo_optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def read_data(train_path):\n",
    "  sentences = []\n",
    "  with open(train_path, 'rb') as f:\n",
    "    for line in f:\n",
    "      sentences.append(line.rstrip().split())\n",
    "  return sentences\n",
    "\n",
    "\n",
    "def build_dataset(sentences, min_count=0):\n",
    "  count = [['UNK', -1]]\n",
    "  sentences_flat = flatten(sentences)\n",
    "  counter = collections.Counter(sentences_flat)\n",
    "  n = len(counter)\n",
    "  filt = [(word, c) for word, c in counter.most_common(n) if c > min_count]\n",
    "  count.extend(filt)\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for sentence in sentences:\n",
    "    sentence_ids = []\n",
    "    for word in sentence:\n",
    "      if word in dictionary:\n",
    "        index = dictionary[word]\n",
    "      else:\n",
    "        index = 0  # dictionary['UNK']\n",
    "        unk_count += 1\n",
    "      sentence_ids.append(index)\n",
    "    data.append(sentence_ids)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "def build_train_validation(data, validation_fraction=0.1):\n",
    "  vad_idx = np.random.choice(\n",
    "      range(len(data)), int(validation_fraction * len(data)), replace=False)\n",
    "  raw_vad_data = [data[i] for i in vad_idx]\n",
    "  train_data = [data[i] for i in list(set(range(len(data))) - set(vad_idx))]\n",
    "  train_counts = collections.Counter(flatten(train_data))\n",
    "  vad_data = []\n",
    "  for vad_sentence in raw_vad_data:\n",
    "    if any(word not in train_counts for word in vad_sentence):\n",
    "      train_data.append(vad_sentence)\n",
    "    else:\n",
    "      vad_data.append(vad_sentence)\n",
    "  print(f\"\"\"Split data into {len(train_data)} train and {len(vad_data)} \n",
    "        validation\"\"\")\n",
    "  return train_data, vad_data\n",
    "\n",
    "\n",
    "def generate_batch(data, corpus_size, count, subsample=1e-3):\n",
    "  global sentence_index\n",
    "  global words_processed\n",
    "  raw_sentence = data[sentence_index]\n",
    "  if subsample == 0.:\n",
    "    sentence = raw_sentence\n",
    "  else:\n",
    "    sentence = []\n",
    "    for word_id in raw_sentence:\n",
    "      word_freq = count[word_id][1]\n",
    "      keep_prob = ((np.sqrt(word_freq / (subsample * corpus_size)) + 1) *\n",
    "                   (subsample * corpus_size) / word_freq)\n",
    "      if np.random.rand() > keep_prob:\n",
    "        pass\n",
    "      else:\n",
    "        sentence.append(word_id)\n",
    "    if len(sentence) < 2:\n",
    "      sentence = raw_sentence\n",
    "  sentence_index = (sentence_index + 1) % len(data)\n",
    "  return get_sentence_inputs(sentence, len(count))\n",
    "\n",
    "\n",
    "def get_sentence_inputs(sentence, vocabulary_size):\n",
    "  sentence_set = set(sentence)\n",
    "  batch = np.asarray(sentence, dtype=np.int32)\n",
    "  labels = np.asarray(\n",
    "      [list(sentence_set - set([w])) for w in sentence], dtype=np.int32)\n",
    "  return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  raw_data = read_data(train_path)\n",
    "  data, count, dictionary, reverse_dictionary = build_dataset(raw_data)\n",
    "  train_data, vad_data = build_train_validation(data)\n",
    "  vocabulary_size = len(dictionary)\n",
    "  words_per_epoch = len(flatten(train_data))\n",
    "  sentences_per_epoch = len(train_data)\n",
    "  del raw_data # Hint to reduce memory.\n",
    "  print('Most common words (+UNK)', count[:5])\n",
    "  print('Sample data', data[0][:10], [reverse_dictionary[i] for i in data[0][:10]])\n",
    "  global sentence_index\n",
    "  global words_processed\n",
    "  sentence_index = 0\n",
    "  words_processed = 0\n",
    "  print('example batch: ')\n",
    "  batch, labels = generate_batch(data, words_per_epoch, count)\n",
    "  for i in range(len(batch)):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "          '->', [w for w in labels[i]], [reverse_dictionary[w] for w in labels[i]])\n",
    "  valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "  valid_window = 100  # Only pick words in the head of the distribution\n",
    "  valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "  \n",
    "  model = keras.Sequential([\n",
    "      keras.layers.Embedding(vocabulary_size, embedding_size),\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(1, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  words_processed_ph = tf.Variable(initial_value=tf.zeros([1,1], \n",
    "                                                      dtype=tf.int32), \n",
    "                                      validate_shape=False)\n",
    "  words_to_train = float(words_per_epoch * epochs_desired)\n",
    "  lr = learning_rate * tf.maximum(\n",
    "        0.0001, 1.0 - tf.cast(words_processed_ph, tf.float32) / words_to_train)\n",
    "\n",
    "  if algo_optimizer == 'sgd':\n",
    "    model.compile(optimizer=keras.optimizers.SGD(lr), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "  elif algo_optimizer == 'adam':\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.999, epsilon=1e-6), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "  model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into 604206 train and 62715 \n",
      "        validation\n",
      "Most common words (+UNK) [['UNK', 0], (b'[', 39775), (b']', 39775), (b'{', 39774), (b'\"id\":', 39774)]\n",
      "Sample data [1] [b'[']\n",
      "example batch: \n",
      "1 b'[' -> [] []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "initial_value must have a shape specified: Tensor(\"mul:0\", dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-f84db47e9707>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0malgo_optimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     model.compile(optimizer=keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.999, epsilon=1e-6), \n\u001b[0m\u001b[1;32m     44\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                   metrics=['accuracy'])\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lr, beta_1, beta_2, epsilon, decay, amsgrad, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'beta_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'beta_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m    594\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m    597\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[1;32m    231\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[1;32m    360\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitial_value_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             raise ValueError(\"initial_value must have a shape specified: %s\" %\n\u001b[0;32m--> 362\u001b[0;31m                              self._initial_value)\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;31m# If 'initial_value' makes use of other variables, make sure we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: initial_value must have a shape specified: Tensor(\"mul:0\", dtype=float32)"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.add(keras.layers.Embedding(200, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'version' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ba3c691814fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'version' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
